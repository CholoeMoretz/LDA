Google借助计算引擎A2 VMs在云中推出了首批NVIDIA Ampere A100 GPU
原标题：Google借助计算引擎A2 VMs在云中推出了首批NVIDIA Ampere A100 GPU
Google设计了A2虚拟机系列，以提高其客户的培训和推理计算性能。A2具有基于新NVIDIA Ampere架构的NVIDIA A-100 Tensor Core图形处理单元。根据博客文章，A-100的计算性能是上一代GPU的20倍，并配备40 GB的高性能HBM2 GPU内存。此外，A2 VM配备多达96个Intel Cascade Lake vCPU，可选的本地SSD，用于需要更快地将数据馈送到GPU和高达100 Gbps网络的工作负载。
当客户的工作负载要求更高时，A2提供带有16个A100 GPU的a2-megagpu-16g实例，其中包括总共640 GB的GPU内存，1.3 TB的系统内存以及通过NVSwitch连接的所有组件，最高可提供聚合带宽为9.6TB / s。
请注意，A2还提供了较小的配置，使客户可以满足他们对GPU计算能力的需求。客户可以选择五种配置，从一到16个GPU，具有两种不同的CPU和网络对GPU的比率-由于Ampere的多实例组（MIG）功能，每个GPU最多可划分为七个GPU实例。。
NVIDIA加速计算部门总经理兼副总裁Ian Buck在最近的公司博客中写道，A-100在GCP上的可用性：
在云数据中心中，A100可以支持广泛的计算密集型应用程序，包括AI培训和推理，数据分析，科学计算，基因组学，边缘视频分析，5G服务等。
借助A2系列，虚拟机Google进一步扩展了预定义和自定义虚拟机的产品范围，从计算到加速器优化的机器。此外，该公司将继续与其他云竞争厂商如微软，该公司最近发布了新的通用和各种内存优化VM家庭英特尔芯片组（AVX-512） -和AWS，它最近发布了基于其EC2 INF1实例Inferentia筹码。这些新的VM类型中有许多是针对具有AI和机器学习工作负载的客户的。
Constellation Research Inc.首席分析师兼副总裁说：
云领导力之战主要是在AI之战中进行的，而这就是使企业的AI负载吸引到每个供应商的云中。中间是诸如NVidia之类的平台供应商，它们提供跨云平台和内部部署选项。因此，随着Google将最新的Nvidia平台引入其Google Cloud，它使CxO可以更轻松地将AI工作负载跨内部和（Google）云迁移。
随着Google成为排名第三的供应商，它必须在吸引负载方面更加开放和更具创造力-这是Google战略的另一个例子。相比之下，更大的AWS和Azure战略仍将转向AI负载的云专有计算架构。CxO必须意识到锁定对于大多数技术供应商而言仍然是理想的结果，并且需要权衡便利性，速度和锁定之间的风险。
目前，A2 VM系列处于Alpha状态，客户可以通过注册来请求访问权限。此外，谷歌表示，公众可获得性和价格信息将在今年晚些时候发布。最后，该公司还宣布即将为Nvidia A100提供对Google Kubernetes Engine，Cloud AI Platform和其他服务的支持。【TechWeb】
